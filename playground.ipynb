{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a document with German text, which appears to be a request for a change of residence for a person named Berlin. It includes sections for the applicant's name, address, and the reason for the change. There is also a section for the family name and surname, and a space for the date of birth. The document is stamped with the 'Bezirksamt Mitte von Berlin' and has a date of 11.04.2024. At the bottom, there is a seal with the text 'Berlin, 11.04.2024' and a logo that includes a shield with a cross and a banner with the text 'BEZIRKSAMT MITTE'.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "load_data = {\"num_gpus\": 1, \"models_per_gpu\": 1}\n",
    "response = requests.post(\"http://127.0.0.1:8000/load\", json=load_data)\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "import base64\n",
    "\n",
    "def load_image(image_source):\n",
    "    \"\"\"\n",
    "    Load an image from a local file path or a URL, convert it to RGB if necessary,\n",
    "    and return the image as a base64-encoded JPEG.\n",
    "\n",
    "    Args:\n",
    "        image_source (str): Path to the image or URL of the image.\n",
    "\n",
    "    Returns:\n",
    "        str: Base64-encoded JPEG image.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(image_source):  # Check if the source is a local file\n",
    "        image = Image.open(image_source)\n",
    "    else:  # Assume the source is a URL\n",
    "        response = requests.get(image_source)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    \n",
    "    # Convert the image to RGB mode if it is RGBA\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Save the image to a buffer as a JPEG\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    \n",
    "    # Encode the buffer content to base64 and return\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# change to your image\n",
    "uk = \"C:/Users/Mariyan/Downloads/uk_passport.png\"\n",
    "# webp doesn't work\n",
    "togo = \"C:/Users/Mariyan/Downloads/togo_anmeldung_1.webp\"\n",
    "bike=\"https://preview.redd.it/picked-up-a-new-to-me-bike-after-a-4-year-riding-hiatus-v0-xvkmmnl8smjd1.jpeg?auto=webp&s=a0b9ac8e7c96c392d80fc0439dee06af01c63cdc\"\n",
    "prompt = \"whats in the picture\"\n",
    "\n",
    "\n",
    "# Send a prompt\n",
    "prompt_data = {\n",
    "    \"model\": 0, #TODO model_path\n",
    "    \"image\": load_image(togo), # optional, can be None\n",
    "    \"text\": prompt\n",
    "}\n",
    "response = requests.post(\"http://127.0.0.1:8000/prompt\", json=prompt_data)\n",
    "print(response.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a screenshot of a code editor with multiple open files and a terminal window displaying Python code. The code appears to be related to machine learning or AI modeling, as indicated by the references to 'models', 'image_base64', and 'send_screenshot'. There are also mentions of 'PromptRequest', 'model_path', and 'model_name', suggesting a function to process a text prompt using a specified model. The terminal window shows a Python prompt with a function definition and a while loop, which seems to be part of a larger script. The code is likely part of a software development process, possibly for a project involving AI or machine learning applications.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from PIL import ImageGrab\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Server URL\n",
    "SERVER_URL = \"http://127.0.0.1:8000/prompt\"\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def append_to_daily_observations(model_response: str, file_path: str = \"daily_observations.txt\"):\n",
    "    \"\"\"\n",
    "    Appends the model's response to a local .txt file in CSV format, recording the current datetime and response text.\n",
    "\n",
    "    Parameters:\n",
    "    model_response (str): The text generated by the model.\n",
    "    file_path (str): The path to the file where the observations will be recorded. Defaults to 'daily_observations.txt'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Prepare the row to be appended\n",
    "    row = [current_datetime, model_response]\n",
    "\n",
    "    # Append the row to the CSV file\n",
    "    with open(file_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# model_response = \"This is a sample observation from the model.\"\n",
    "# append_to_daily_observations(\" The image displays an e-meeting interface with various software applications like Explorer, Visual Studio Code, and Docker running. There's an overlay instruction for the user to analyze the current screenshot, listing elements such as applications and tools currently open, visual content, conversations and messages, task lists and reminders, and unusual elements. The user is prompted to note down insights from the analysis. The user's task analysis process includes evaluating the open applications, visual content, ongoing conversations, task lists, emails, and any out-of-the-ordinary elements to understand the current work engagement. The provided insights may indicate the user's mental and task load, potential distractions, or the alignment of their work tasks with priorities. Additionally, the user may offer suggestions or point out issues like an unaligned mood from a recent meeting, to improve the productivity or work-life balance. The key focus areas will ultimately help tailor personal productivity enhancing strategies.\")\n",
    "\n",
    "def take_screenshot():\n",
    "    \"\"\"Take a screenshot of the current screen.\"\"\"\n",
    "    screenshot = ImageGrab.grab()\n",
    "    return screenshot\n",
    "\n",
    "def encode_image_to_base64(image):\n",
    "    \"\"\"Convert the screenshot to base64.\"\"\"\n",
    "    # Convert RGBA to RGB if necessary\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "prompt = '''\n",
    "You are Mariyan, as you analyze this current screenshot, focus on the information and \n",
    "tasks directly in front of you. What does this snapshot reveal about your current \n",
    "state of mind, priorities, or focus? Consider the applications, documents, or \n",
    "conversations that are open on your screen right now. What do they suggest about \n",
    "your immediate goals or concerns? Provide insights on how this moment fits into \n",
    "the larger picture of your day, but emphasize the unique aspects of what’s \n",
    "happening right now. What can you learn or infer from this specific moment?\n",
    "'''\n",
    "\n",
    "structured = '''\n",
    "\n",
    "image_context : \n",
    "main_activity:\n",
    "\n",
    "'''\n",
    "\n",
    "focused = '''\n",
    "Mariyan, as you analyze this current image of your daily life, focus on extracting the most curious and insightful information from what’s in front of you. Pay close attention to:\n",
    "\n",
    "\t1.\tApplications and Tools: What applications or tools are currently open? What does their presence suggest about your current tasks, priorities, or state of mind? Are these tools aligned with your immediate goals or do they hint at a distraction?\n",
    "\t2.\tVisual Content: If there are photos or non-text visuals, what do they depict? How do these images relate to your day-to-day life, and what might they reveal about your personal interests or current focus areas?\n",
    "\t3.\tConversations and Messages: Are there any open chat windows, emails, or other forms of communication? What do these interactions suggest about your current social or professional engagements? What tone or themes are evident in these conversations?\n",
    "\t4.\tTask Lists and Reminders: If there are any visible to-do lists, reminders, or schedules, what do they indicate about your immediate priorities or concerns? Are there any overdue tasks or upcoming deadlines that stand out?\n",
    "\t5.\tUnusual Elements: Identify anything that seems out of place or particularly interesting. What could these anomalies reveal about your current environment, mindset, or recent activities?\n",
    "\n",
    "Based on the analysis, provide a summary of the insights that can be drawn from this moment. How do these elements fit together, and what can you infer about your current state of mind, focus, or goals?\n",
    "\n",
    "'''\n",
    "def send_screenshot(image_base64):\n",
    "    \"\"\"Send the base64-encoded screenshot to the server.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": 0,\n",
    "        \"image\": image_base64,\n",
    "        \"text\": \"what is this\",\n",
    "        \"audio\": None,\n",
    "        \"model_name\": \"phi\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(SERVER_URL, json=payload)\n",
    "    except Exception as e:\n",
    "        print(\"Error sending screenshot:\", e.content)\n",
    "\n",
    "    print(response.json()['response'])\n",
    "\n",
    "    return f\"{response.json()}\"\n",
    "\n",
    "def send_audio(audio: str):\n",
    "    \"\"\"Send the base64-encoded audio to the server.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": 1,\n",
    "        \"image\": None,\n",
    "        \"text\": \"Summarise this\",\n",
    "        \"audio\": audio,\n",
    "        \"model_name\": \"whisper\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(SERVER_URL, json=payload)\n",
    "    except Exception as e:\n",
    "        print(\"Error sending screenshot:\", e)\n",
    "\n",
    "    print(response.json()['response'])\n",
    "\n",
    "    return f\"{response.json()}\"\n",
    "\n",
    "\n",
    "while True:\n",
    "        # IDEA: add a keyboard shortcut for highlight moment    \n",
    "        # Take a screenshot\n",
    "        screenshot = take_screenshot()\n",
    "        \n",
    "        # Encode the image to base64\n",
    "        image_base64 = encode_image_to_base64(screenshot)\n",
    "        \n",
    "        # Send the image to the server\n",
    "        response = send_screenshot(image_base64)\n",
    "        \n",
    "        append_to_daily_observations(response)\n",
    "        \n",
    "        # Wait for 5 minutes between screenshots\n",
    "        time.sleep(60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for sound...\n",
      "Sound detected, starting recording...\n",
      "Silence detected, stopping recording...\n",
      "Recording saved to 2024-08-21_14-43-39_recording.wav\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "import datetime\n",
    "\n",
    "def is_silent(audio_segment, silence_thresh=-50.0, min_silence_len=3000):\n",
    "    \"\"\"Return True if the given audio segment is silent.\"\"\"\n",
    "    non_silent_ranges = detect_nonsilent(audio_segment, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "    return len(non_silent_ranges) == 0\n",
    "\n",
    "def record_audio_on_sound(threshold=500, chunk_size=1024, sample_format=pyaudio.paInt16, channels=1, rate=44100, silence_duration=2):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # List all audio devices and find BlackHole\n",
    "    for i in range(p.get_device_count()):\n",
    "        dev_info = p.get_device_info_by_index(i)\n",
    "        if \"BlackHole\" in dev_info['name']:\n",
    "            device_index = i\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"BlackHole device not found.\")\n",
    "\n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=rate,\n",
    "                    input=True,\n",
    "                    input_device_index=device_index,\n",
    "                    frames_per_buffer=chunk_size)\n",
    "\n",
    "    print(\"Listening for sound...\")\n",
    "    frames = []\n",
    "    recording = False\n",
    "    silence_frames = 0\n",
    "\n",
    "    sample_width = p.get_sample_size(sample_format)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(chunk_size)\n",
    "            audio_segment = AudioSegment(data, sample_width=sample_width, frame_rate=rate, channels=channels)\n",
    "\n",
    "            # Convert data to numpy array for processing\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "            if np.abs(audio_data).mean() > threshold:\n",
    "                if not recording:\n",
    "                    print(\"Sound detected, starting recording...\")\n",
    "                    recording = True\n",
    "                frames.append(data)\n",
    "                silence_frames = 0\n",
    "            elif recording:\n",
    "                silence_frames += 1\n",
    "                if silence_frames >= silence_duration * (rate // chunk_size):\n",
    "                    print(\"Silence detected, stopping recording...\")\n",
    "                    break\n",
    "                frames.append(data)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    if recording:\n",
    "        filename = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_recording.wav\"\n",
    "        wf = wave.open(filename, 'wb')\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(sample_width)\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        print(f\"Recording saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No sound detected, no recording made\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio_on_sound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
